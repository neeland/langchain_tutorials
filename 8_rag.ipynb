{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RAG - Part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "* `Part 1` : **simple Q&A application** over **unstructured data**\n",
    "* `Part 2` : extends to accommodate **conversation-style interactions** & **multi-step retrieval** processes\n",
    "  * typical Q&A architecture\n",
    "  * additional resources for more advanced Q&A techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### ~Setup~ ###\n",
    "###############\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load secrets from file\n",
    "with open('secrets.txt') as f:\n",
    "    for line in f:\n",
    "        if '=' in line:\n",
    "            key, value = line.strip().split('=', 1)\n",
    "            os.environ[key] = value\n",
    "\n",
    "# Initialize LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG has 2 parts:\n",
    "\n",
    "1. Indexing: pipeline for ingesting data from source & indexing it (usually happens offline)\n",
    "\n",
    "2. Retrieval & generation: actual **RAG chain**: \n",
    "   * takes user query at run time\n",
    "   * retrieves  relevant data from index \n",
    "   * passes that to `model`\n",
    "\n",
    "> ℹ **Note**: indexing portion of this tutorial will largely follow the `3_retreivers.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common full sequence from raw data to answer:\n",
    "\n",
    "#### 1️⃣ Indexing\n",
    "---\n",
    "1. **Load**: Done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/).\n",
    "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) break large Documents into smaller chunks. Useful both for indexing data & passing it into model: **large chunks are harder to search over & won't fit in a model's finite context window**\n",
    "3. **Store**: Need somewhere to store & index our splits, so that they can be searched over later. Often done usin [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) & [Embeddings models](https://python.langchain.com/docs/concepts/embedding_models/).\n",
    "\n",
    "![Image URL](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
    "\n",
    "#### 2️⃣ Retreival & Generation\n",
    "---\n",
    "4. Retrieve: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers/)\n",
    "5. Generate: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/) **produces an answer** using prompt that includes **both the question with retrieved data**\n",
    "![Image URL](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
    "\n",
    "> Once *data indexed*, we will use `LangGraph` as **orchestration framework** to RAG steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### `InMemoryVectorStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "\n",
    "Build app that answers questions about the website's content\n",
    "Specific website: [**LLM Powered Autonomous Agents blog post by Lilian Weng**](https://lilianweng.github.io/posts/2023-06-23-agent/) → ask questions about the contents\n",
    "\n",
    "> **Simple indexing pipeline & RAG chain in ~50 lines of code**\n",
    "\n",
    "> **API Reference**: [`hub`]() | [`WebBaseLoader`]() | [`Document`]() | [`RecursiveCharacterTextSplitter`]() | [`StateGraph`]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke(\n",
    "        {\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate better planning and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used to enhance model performance by encouraging step-by-step reasoning. This can be achieved through simple prompting, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed walkthrough\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "> Abbreviated version of `3_retreiver.ipynb`\n",
    "\n",
    "#### Loading documents\n",
    "* Load blog post contents\n",
    "* [`DocumentLoaders`]() are objects that load in data from source returing list of [`Document`]() objects\n",
    "  * In this example, [`WebBaseLoader`]() uses `urllib` to load HTML from web URL & `BeautifulSoup` to parse it to text\n",
    "  * Customize HTML parsing in param in BS `BeautifulSoup` parser `bs_kwargs`\n",
    "  * In this example, only HTML tags with class `“post-content”`, `“post-title”`, or `“post-header”` relevant, so we’ll remove all others\n",
    "\n",
    "> **API Ref** [`WebBaseLoader`]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43130\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(\n",
    "    class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DocumentLoader`: Object that loads data from a source as list of Documents.\n",
    "\n",
    "* [Docs](https://python.langchain.com/docs/how_to/#document-loaders): Detailed documentation on how to use DocumentLoaders.\n",
    "* [Integrations](https://python.langchain.com/docs/integrations/document_loaders/): 160+ integrations to choose from.\n",
    "* [Interface](https://python.langchain.com/docs/tutorials/rag/#:~:text=160%2B%20integrations%20to%20choose%20from.-,Interface,-%3A%20API%20reference%20for%20the%20base): API reference for the base interface.\n",
    "\n",
    "### Splitting documents\n",
    "\n",
    "* Loaded document > 42k characters → too long for context window\n",
    "* Even models that fit full post in context window, **models struggle to find information in long inputs**\n",
    "* →  split `Document` into chunks for embedding & vector storage\n",
    "  * Should help retreive only most relevant parts of blog post at run time\n",
    "\n",
    "As in `3_retreivers.ipynb` use `RecursiveCharacterTextSplitter` \n",
    "    * --> Recursively split document using common separators (new lines etc) until each chunk is  appropriate size (recommended text splitter for generic text use cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 66 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go deeper\n",
    "`TextSplitter`: Object that splits list of `Documents` into smaller chunks. Subclass of `DocumentTransformers`.\n",
    "\n",
    "Learn more about splitting text using different methods by reading the how-to docs\n",
    "* [Code (`py` or `js`)](https://python.langchain.com/docs/integrations/document_loaders/source_code/)\n",
    "* [Scientific papers](https://python.langchain.com/docs/integrations/document_loaders/grobid/)\n",
    "* [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface\n",
    "\n",
    "`DocumentTransformer`: Object that performs a **transformation on a list of Document objects**.\n",
    "\n",
    "  * [Docs](https://python.langchain.com/docs/how_to/#text-splitters): Detailed documentation on how to use `DocumentTransformers`\n",
    "  * [Integrations](https://python.langchain.com/docs/integrations/document_transformers/)\n",
    "  * [Interface](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.transformers.BaseDocumentTransformer.html): API reference for the base interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
